{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b6c098",
   "metadata": {},
   "source": [
    "## Data Analysis and AI ML Model Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc748e2",
   "metadata": {},
   "source": [
    " ##### Welcome! This is my project called -> Predicting Ultra-Marathon Performance, Data Driven Approach using Specific Marathon Event trends and Athlete metadata\n",
    "\n",
    " Made by Bryan Lee Santoso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a626a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "# The dataset is assumed to be in the same directory as this script\n",
    "df = pd.read_csv(\"TWO_CENTURIES_OF_UM_RACES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it contains 7,461,226 ultra-marathon race records from 1,641,168 athletes, we will just sample a bit\n",
    "df_sample = df.sample(frac=0.10, random_state=42)  # 10% sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the column types\n",
    "# Convert dates and extract features\n",
    "df_sample['Event dates'] = pd.to_datetime(df_sample['Event dates'], errors='coerce')\n",
    "df_sample['Event_year'] = df_sample['Event dates'].dt.year\n",
    "df_sample['Event_season'] = df_sample['Event dates'].dt.quarter  # 1=Winter, 2=Spring, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf724a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up null values for data validity\n",
    "df_sample.isnull().sum()\n",
    "df_sample = df_sample.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['Athlete performance'] = df_sample['Athlete performance'].str.split(' ').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245600d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.drop(['Athlete club'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a74b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d687e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "print(df_sample.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the athlete performance to total seconds\n",
    "df_sample['Athlete performance'] = pd.to_timedelta(df_sample['Athlete performance'].str.replace(' h', ''), errors='coerce').dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample[df_sample['Event distance/length'].isin(['50km', '100km'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric distance from the string and convert to float\n",
    "df_sample['Event distance/length'] = df_sample['Event distance/length'].str.replace('km', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['Athlete average speed'] = pd.to_numeric(df_sample['Athlete average speed'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.isnull().sum()\n",
    "df_sample = df_sample.dropna(subset = ['Athlete average speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate athlete's age at the event\n",
    "df_sample['Athlete age'] = df_sample['Year of event'] - df_sample['Athlete year of birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract country from event name\n",
    "df_sample['Event country'] = df_sample['Event name'].str.extract(r'\\((.*?)\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69662a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(df_sample.duplicated().sum())\n",
    "\n",
    "# Drop duplicates\n",
    "df_sample = df_sample.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checking of the data types\n",
    "print(df_sample.dtypes)\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8072882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for representation, since we have two categories of races, 50km and 100km, we can view the statistics of each categories by seperating them first\n",
    "df_50km = df_sample[df_sample['Event distance/length'] == 50]\n",
    "df_100km = df_sample[df_sample['Event distance/length'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288293ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Event distance/length', y='Athlete performance', data=df_50km)\n",
    "plt.show()\n",
    "sns.boxplot(x='Event distance/length', y='Athlete performance', data=df_100km)\n",
    "plt.show()\n",
    "\n",
    "df_50km['Athlete performance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8857479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are outliers, we want to clean up the data first before continuing, we will clean up each category first, and then merge the dataset back together in order to prepare the final dataset for the model. We are focusing primarily on athlete performance\n",
    "\n",
    "# First we want to clean the 50km dataset first\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "\n",
    "Q1_50 = df_50km['Athlete performance'].quantile(0.25)\n",
    "Q3_50 = df_50km['Athlete performance'].quantile(0.75)\n",
    "\n",
    "# Calculate the IQR\n",
    "IQR_50 = Q3_50 - Q1_50\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound_50 = Q1_50 - 1.5 * IQR_50\n",
    "upper_bound_50 = Q3_50 + 1.5 * IQR_50\n",
    "\n",
    "print(f\"Lower Bound (50km): {lower_bound_50}, Upper Bound (50km): {upper_bound_50}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we want to calculate the statitics for the 100km races\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "\n",
    "Q1_100 = df_100km['Athlete performance'].quantile(0.25)\n",
    "Q3_100 = df_100km['Athlete performance'].quantile(0.75)\n",
    "\n",
    "# Calculate the IQR\n",
    "IQR_100 = Q3_100 - Q1_100\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound_100 = Q1_100 - 1.5 * IQR_100\n",
    "upper_bound_100 = Q3_100 + 1.5 * IQR_100\n",
    "\n",
    "print(f\"Lower Bound (100km): {lower_bound_100}, Upper Bound (100km): {upper_bound_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we remove the outliers based on the IQR rule, it will significantly reduce the dataset, thus we will take Winsorize the outliers , capping the outliers at the 2nd and 98th percentiles\n",
    "# Cap outliers at the 2nd and 98th percentiles\n",
    "lower_cap_50 = df_50km['Athlete performance'].quantile(0.02)\n",
    "upper_cap_50 = df_50km['Athlete performance'].quantile(0.98)\n",
    "\n",
    "df_50km['Athlete performance'] = df_50km['Athlete performance'].clip(lower=lower_cap_50, upper=upper_cap_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Event distance/length', y='Athlete performance', data=df_50km)\n",
    "plt.show()\n",
    "df_50km.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we remove the outliers based on the IQR rule, it will significantly reduce the dataset, thus we will take Winsorize the outliers , capping the outliers at the 2nd and 98th percentiles\n",
    "# Cap outliers at the 2nd and 98th percentiles\n",
    "lower_cap_100 = df_100km['Athlete performance'].quantile(0.02)\n",
    "upper_cap_100 = df_100km['Athlete performance'].quantile(0.98)\n",
    "\n",
    "df_100km['Athlete performance'] = df_100km['Athlete performance'].clip(lower=lower_cap_100, upper=upper_cap_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Event distance/length', y='Athlete performance', data=df_100km)\n",
    "plt.show()\n",
    "df_100km.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After data cleaning, we will concatenate the two datasets back together\n",
    "final_df = pd.concat([df_50km, df_100km], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop unrelated columns that are not needed for the model\n",
    "final_df = final_df.drop(['Year of event', 'Event dates', 'Athlete year of birth', 'Athlete average speed', 'Athlete ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73abfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(['Event number of finishers', 'Athlete country', 'Athlete country', 'Athlete age category','Event_year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdce474",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)\n",
    "final_df['Event name'] = final_df['Event name'].str.split('(').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14298d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning the ML model Traning\n",
    "# Idea -> By inputting the Athlete age and gender, Event name, Event distance, and Event season, we are able to predict the Athlete's performance\n",
    "# Constrains, models are only predicting in races Kilometer race cetegories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dcab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we do feature engineering through target encoding\n",
    "# Calculate the average performance for each event\n",
    "event_performance_mean = final_df.groupby('Event name')['Athlete performance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the event name with its mean performance\n",
    "final_df['Event name (encoded)'] = final_df['Event name'].map(event_performance_mean)\n",
    "final_df.head(10)\n",
    "final_df = final_df.drop(['Event country'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To prevent information leakage, split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = final_df.drop(columns=['Athlete performance'])\n",
    "y = final_df['Athlete performance']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate target encoding based on training data\n",
    "event_performance_mean = X_train.merge(y_train, left_index=True, right_index=True).groupby('Event name')['Athlete performance'].mean()\n",
    "\n",
    "X_train['Event name (encoded)'] = X_train['Event name'].map(event_performance_mean)\n",
    "X_test['Event name (encoded)'] = X_test['Event name'].map(event_performance_mean)\n",
    "\n",
    "# Handle unseen events in the test set (fallback to global mean if the input event was not included in the original dataset) \n",
    "global_mean = y_train.mean()\n",
    "X_test['Event name (encoded)'] = X_test['Event name (encoded)'].fillna(global_mean)\n",
    "\n",
    "X_train = X_train.drop(columns=['Event name'])\n",
    "X_test = X_test.drop(columns=['Event name'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "X_train['Athlete gender'] = label_encoder.fit_transform(X_train['Athlete gender'])\n",
    "X_test['Athlete gender'] = label_encoder.transform(X_test['Athlete gender'])\n",
    "\n",
    "final_df = final_df.drop(columns=['Event name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96917772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Create non-linear features\n",
    "X_train['Age_squared'] = X_train['Athlete age'] ** 2  # Capturing non-linear age effects\n",
    "X_test['Age_squared'] = X_test['Athlete age'] ** 2\n",
    "\n",
    "# Interaction terms\n",
    "X_train['Age_gender_interaction'] = X_train['Athlete age'] * X_train['Athlete gender']\n",
    "X_test['Age_gender_interaction'] = X_test['Athlete age'] * X_test['Athlete gender']\n",
    "\n",
    "X_train['Distance_age_interaction'] = X_train['Event distance/length'] * X_train['Athlete age']\n",
    "X_test['Distance_age_interaction'] = X_test['Event distance/length'] * X_test['Athlete age']\n",
    "\n",
    "# Re-train your best model with these new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c74671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values of athlete performance are continuous data, then we will use a simple Random Forest Regressor\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb867e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11153628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"IMPROVING RANDOM FOREST PERFORMANCE\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Adding non-linear features and interactions...\")\n",
    "\n",
    "X_train_enhanced = X_train.copy()\n",
    "X_test_enhanced = X_test.copy()\n",
    "X_train_enhanced['Age_squared'] = X_train['Athlete age'] ** 2\n",
    "X_test_enhanced['Age_squared'] = X_test['Athlete age'] ** 2\n",
    "X_train_enhanced['Age_group'] = pd.cut(X_train['Athlete age'], bins=[0, 30, 40, 50, 100], labels=[0, 1, 2, 3])\n",
    "X_test_enhanced['Age_group'] = pd.cut(X_test['Athlete age'], bins=[0, 30, 40, 50, 100], labels=[0, 1, 2, 3])\n",
    "X_train_enhanced['Distance_X_Age'] = X_train['Event distance/length'] * X_train['Athlete age']\n",
    "X_test_enhanced['Distance_X_Age'] = X_test['Event distance/length'] * X_test['Athlete age']\n",
    "X_train_enhanced['Gender_X_Age'] = X_train['Athlete gender'] * X_train['Athlete age']\n",
    "X_test_enhanced['Gender_X_Age'] = X_test['Athlete gender'] * X_test['Athlete age']\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],  \n",
    "    'min_samples_split': [2, 5],  \n",
    "    'min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  \n",
    "    n_jobs=-1,  \n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "print(\"\\nTuning Random Forest with original features...\")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf_original = rf_grid.best_estimator_\n",
    "best_rf_original_pred = best_rf_original.predict(X_test)\n",
    "best_rf_original_r2 = r2_score(y_test, best_rf_original_pred)\n",
    "best_rf_original_mae = mean_absolute_error(y_test, best_rf_original_pred)\n",
    "best_rf_original_mse = mean_squared_error(y_test, best_rf_original_pred)\n",
    "\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"R² Score: {best_rf_original_r2}\")\n",
    "print(f\"MAE: {best_rf_original_mae}\")\n",
    "print(f\"MSE: {best_rf_original_mse}\")\n",
    "\n",
    "print(\"\\nTuning Random Forest with enhanced features...\")\n",
    "rf_grid.fit(X_train_enhanced, y_train)\n",
    "best_rf_enhanced = rf_grid.best_estimator_\n",
    "best_rf_enhanced_pred = best_rf_enhanced.predict(X_test_enhanced)\n",
    "best_rf_enhanced_r2 = r2_score(y_test, best_rf_enhanced_pred)\n",
    "best_rf_enhanced_mae = mean_absolute_error(y_test, best_rf_enhanced_pred)\n",
    "best_rf_enhanced_mse = mean_squared_error(y_test, best_rf_enhanced_pred)\n",
    "\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"R² Score: {best_rf_enhanced_r2}\")\n",
    "print(f\"MAE: {best_rf_enhanced_mae}\")\n",
    "print(f\"MSE: {best_rf_enhanced_mse}\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"\\nTraining Gradient Boosting with original features...\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_r2 = r2_score(y_test, gb_pred)\n",
    "gb_mae = mean_absolute_error(y_test, gb_pred)\n",
    "gb_mse = mean_squared_error(y_test, gb_pred)\n",
    "\n",
    "print(f\"R² Score: {gb_r2}\")\n",
    "print(f\"MAE: {gb_mae}\")\n",
    "print(f\"MSE: {gb_mse}\")\n",
    "\n",
    "print(\"\\nMODEL COMPARISON:\")\n",
    "print(\"-\" * 50)\n",
    "models = {\n",
    "    \"Linear Regression (Baseline)\": [baseline_r2, baseline_mae, baseline_mse],\n",
    "    \"Random Forest (Default)\": [r2, mae, mse],\n",
    "    \"Random Forest (Tuned)\": [best_rf_original_r2, best_rf_original_mae, best_rf_original_mse],\n",
    "    \"Random Forest (Enhanced Features)\": [best_rf_enhanced_r2, best_rf_enhanced_mae, best_rf_enhanced_mse],\n",
    "    \"Gradient Boosting\": [gb_r2, gb_mae, gb_mse]\n",
    "}\n",
    "\n",
    "try:\n",
    "    models[\"XGBoost\"] = [xgb_r2, xgb_mae, xgb_mse]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"{'Model':<30} {'R²':<10} {'MAE':<10} {'MSE':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for model_name, metrics in models.items():\n",
    "    print(f\"{model_name:<30} {metrics[0]:<10.4f} {metrics[1]:<10.2f} {metrics[2]:<15.2f}\")\n",
    "\n",
    "best_model = max(models.items(), key=lambda x: x[1][0])\n",
    "print(f\"\\nBest model: {best_model[0]} with R² = {best_model[1][0]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(models.keys(), [m[0] for m in models.values()])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e139be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(model, 'athlete_performance_model.pkl')\n",
    "# loaded_model = joblib.load('athlete_performance_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "user_input = {\n",
    "    'Event name': 'Flatland Marathon',\n",
    "    'Event distance/length': 50,\n",
    "    'Event season': 2,\n",
    "    'Athlete age': 30,\n",
    "    'Athlete gender': 'M'\n",
    "}\n",
    "\n",
    "# Preprocess the user input\n",
    "user_input_encoded = user_input.copy()\n",
    "user_input_encoded['Event name (encoded)'] = event_performance_mean.get(\n",
    "    user_input['Event name'], global_mean \n",
    ")\n",
    "user_input_encoded.pop('Event name')\n",
    "user_input_encoded['Athlete gender'] = label_encoder.transform([user_input['Athlete gender']])[0]\n",
    "user_input_df = pd.DataFrame([user_input_encoded])\n",
    "user_input_df = user_input_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "\n",
    "user_prediction = model.predict(user_input_df)\n",
    "print(f\"Predicted Athlete Performance: {user_prediction[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
